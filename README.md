<!-- ============================================================
Pulse README with "safe" GitHub animations (SVG/GIF only, no JS)
Paste this as README.md
============================================================ -->

<p align="center">
  <img src="assets/cosmic_pulse.gif" alt="Pulse Logo" width="900">
</p>

<p align="center">
  <img
    src="https://capsule-render.vercel.app/api?type=soft&height=60&color=0:ff2d55,100:7a00ff&text=Pulse%20ML%20Pipeline:%20Real-time%20Outlier%20Detection&fontSize=36&fontColor=ffffff"
    alt="Pulse ML Pipeline ‚Äì Real-time Outlier Detection"
  />
</p>

<p align="center">
  <em>Don't let unoptimized queries and black-box algorithms swallow your cloud database budget like a black hole.<br/>
  Pulse brings real-time ML outlier detection to the edge of your data warehouse ‚Äî so you can see what's really going on before it's too late.</em>
</p>

<br/>

Production-ready Kafka streaming pipeline with real-time ML inference and asynchronous model training.

## üí° Why Pulse?

- **üöÄ Production-Ready ONNX Inference** ‚Äî Optimized ML inference with ONNX Runtime supporting both CPU and GPU execution providers
- **üì¶ Optimized File Format** ‚Äî Apache Arrow Feather format with LZ4/ZSTD compression (47x faster writes than Parquet)
- **‚ö° Fully Async Training** ‚Äî Model training runs in a detached subprocess (~90s average), never blocking real-time inference
- **üîÑ Automatic Model Lifecycle** ‚Äî Daily retraining with rolling window data; new models automatically loaded for next inference cycle
- **üìä End-to-End Batch Processing** ‚Äî Batched writes, reads, inference, and training for maximum throughput
- **üõ†Ô∏è Optimized Libraries** ‚Äî orjson (3.6x faster JSON), PyArrow (columnar storage), pandas (vectorized ops)
- **üì° Kafka Streaming** ‚Äî Reliable, persistent message exchange with consumer groups and offset management
- **üìà High Throughput** ‚Äî 17,700 msg/s sustained throughput (300x+ real-time capacity)
- **üîß Parallel Inference** ‚Äî Configurable worker pool for multi-core CPU utilization
- **üê≥ One-Command Docker Deployment** ‚Äî Full stack (Kafka + Pipeline + Dashboard) with `docker compose up`

---

## üöÄ Quick Start (Docker)

> **üìñ Full Docker reference:** [docs/DOCKER.md](docs/DOCKER.md) ‚Äî building locally, clean runs, environment variables, terminal options
>
> **üìñ Manual setup (no Docker):** [docs/MANUAL_SETUP.md](docs/MANUAL_SETUP.md) ‚Äî Python venv, individual component commands, run scripts

### 1. Clone and Get Data

```bash
git clone https://github.com/IvanYachUkr/Pulse.git
cd Pulse
```

Place a **date-sorted** Parquet file from the [Amazon Redset](https://github.com/amazon-science/redset) dataset at:

```
_data/input/sorted_4days.parquet
```

### 2. Start

```bash
docker compose up
```

This pulls the pre-built image, starts Kafka, and runs the entire pipeline. Wait 30‚Äì60 seconds for data to flow, then open the dashboard.

### 3. Access

| Service | URL |
|---------|-----|
| **Dashboard** | [http://localhost:8507](http://localhost:8507) |
| **Kafka Console** | [http://localhost:8080](http://localhost:8080) |

### 4. Stop

```bash
docker compose down
```

> **‚ö†Ô∏è Important:** Close the dashboard browser tab **before** restarting the pipeline from scratch. An open dashboard holds connections to SQLite/DuckDB databases inside the container, which can prevent proper cleanup on restart.

---

## üéØ Overview

This pipeline processes query events from Kafka, detects outliers using ONNX models, and automatically retrains models daily. Optimized for high throughput with Apache Arrow Feather format achieving **47x faster writes** than Parquet.

### Data Flow

```
Kafka ‚Üí Consumer ‚Üí Feather Writer ‚Üí Daily Files (.arrow)
                 ‚Üì
              Inference (ONNX)
                 ‚Üì
           Anomaly Detection
                 
Daily Files ‚Üí Async Training (subprocess) ‚Üí New Models ‚Üí Next Day Inference
```

### Key Features

* **Real-time inference:** Batch inference with ONNX runtime
* **Asynchronous training:** Non-blocking model retraining in subprocess (~90s average)
* **High performance:** 17,700 msg/s throughput (300x+ real-time capacity)
* **Optimized storage:** Apache Arrow Feather format (47x faster writes than Parquet-gzip)
* **Compression options:** LZ4 (maximum speed) or ZSTD (2x better compression)
* **Fast JSON parsing:** orjson (3.6x faster than stdlib)

---

## üìä Query Classification

Every query is classified into one of five categories based on its performance bottleneck:

| Class | What It Captures | Key Threshold |
|-------|-----------------|---------------|
| **Network-bound** | COPY/UNLOAD data transfers | exec > 500ms |
| **CPU-bound** | Complex compute-heavy queries | exec/scan ratio > 20, exec > 1s |
| **IO-bound** | Large scans bottlenecked by disk | scan ‚â• 2 GB, ratio < 0.061 |
| **Queue/WLM-bound** | Simple queries stuck waiting | queue ‚â• exec, complexity < 4 |
| **Normal** | No dominant bottleneck | Everything else |

All thresholds are **data-derived** from behavioral regime analysis of 433M queries ‚Äî not hardcoded assumptions. See **[QUERY_CLASSIFICATION.md](docs/QUERY_CLASSIFICATION.md)** for the full methodology, transition points, and validation results.

---

## üìä Dashboard Architecture

The dashboard is a **decoupled SPA** built with a **FastAPI JSON backend** and a **Preact + HTM** client ‚Äî no build tools, no bundler, no `node_modules`.

### Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Backend** | FastAPI + Uvicorn | JSON API serving dashboard data from SQLite/DuckDB |
| **Frontend** | Preact + HTM (via CDN) | Reactive UI with hooks, delivered as a single HTML file |
| **Charts** | Chart.js (via CDN) | Interactive time-series and classification charts |
| **Styling** | Vanilla CSS (custom) | Dark-mode glassmorphism design with CSS variables |

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/status` | GET | Pipeline component health status |
| `/api/instances` | GET | Available cluster instance IDs |
| `/api/instances/critical` | GET | Instances flagged as critical |
| `/api/metrics?ids=...` | GET | Aggregated performance metrics |
| `/api/classification/chart?ids=...` | GET | Query classification time-series |
| `/api/classification/table?ids=...` | GET | Query classification breakdown |
| `/api/anomalies?ids=...` | GET | Detected anomalous queries |
| `/api/critical/types?ids=...` | GET | Critical problem type distribution |
| `/api/recommendations/{type}` | GET | Auto-generated remediation advice |

### Key Features

* **Multi-instance selection** ‚Äî Select/deselect individual instances, use "Select All" / "Critical Only" shortcuts
* **Time window toggle** ‚Äî Switch between 24h and 1-week views
* **Real-time status indicators** ‚Äî Live health checks for stream analytics, anomaly detection, and database
* **Metric cards** ‚Äî Score, queue time, spillage, anomaly count with trend indicators
* **Interactive charts** ‚Äî Stacked area chart for query classification over time
* **Anomaly table** ‚Äî Detected anomalous queries with severity highlighting
* **Recommendations panel** ‚Äî Context-aware remediation advice based on dominant problem types

---

## üìä Performance

### Overall Pipeline

| Component | Throughput | Notes |
|-----------|-----------|-------|
| **Producer** | 31-38k msg/s | Consistent performance |
| **Consumer (with inference)** | 17,700 msg/s | 4 workers, batch 65k |
| **Consumer (write only)** | 21,600 msg/s | Excluding inference |
| **Training** | ~90s/day | 400k rows per day (async) |

**Real-time capacity:** ~36 msg/s (300x+ speedup exceeds requirements)

### Inference Parallelization Benchmark (600k messages)

| Workers | CPU Throughput | GPU Throughput |
|---------|---------------|---------------|
| **1** | 11,739 msg/s | 10,229 msg/s |
| **4** | **12,864 msg/s** ‚≠ê | 11,248 msg/s |
| **8** | 12,724 msg/s | ‚Äî |

**Recommendation:** Use `--inference-workers 4` for optimal performance (CPU).

> ‚ö†Ô∏è **GPU Note:** Tested with RTX 4070. GPU is **SLOWER** than CPU for this workload! The ONNX Isolation Forest model requires CPU‚ÜîGPU data transfers (memcpy) that negate any compute gains. Use CPU.

---

## üîß Design Decisions & Benchmarks

### Apache Arrow Feather Format (47x Faster Writes)

| Format | Write Speed | Read Speed | File Size |
|--------|------------|-----------|-----------|
| Parquet (gzip) | 1x (baseline) | **5x faster** | **Smallest** |
| **Feather (LZ4)** | **47x faster** | 1x | 7x larger |

**Writes happen in the main consumer loop** (directly impacts throughput). **Reads happen asynchronously** in a subprocess. Therefore, write speed is critical.

### Compression Options (LZ4 vs ZSTD)

| Compression | Write Speed | File Size | Use Case |
|-------------|-----------|-----------|----------|
| **LZ4** (default) | Fastest | Larger (2.3x ratio) | Real-time streaming |
| **ZSTD** | 40% slower | **2x smaller** (5.6x ratio) | Disk-constrained, archival |

### JSON Parsing with orjson (3.6x Faster)

| Library | Speed | Improvement |
|---------|-------|-------------|
| json (stdlib) | 1x (baseline) | - |
| **orjson** | **3.6x faster** | 260% faster |

---

## üìÅ Project Structure

```
Pulse/
‚îú‚îÄ‚îÄ README.md                       # This file
‚îú‚îÄ‚îÄ LICENSE                         # MIT License
‚îú‚îÄ‚îÄ docker-compose.yml              # Full stack Docker setup
‚îú‚îÄ‚îÄ Dockerfile                      # Pipeline container image
‚îú‚îÄ‚îÄ docker-entrypoint.sh            # Container startup script
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ run_pipeline.bat / .sh          # Manual pipeline launchers
‚îú‚îÄ‚îÄ stop_pipeline.bat / .sh         # Manual pipeline stoppers
‚îÇ
‚îú‚îÄ‚îÄ _data/                          # All runtime data (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ input/                      # Source parquet files
‚îÇ   ‚îú‚îÄ‚îÄ arrow/                      # Daily Arrow shard files
‚îÇ   ‚îú‚îÄ‚îÄ models/                     # Trained ONNX + joblib models
‚îÇ   ‚îú‚îÄ‚îÄ logs/                       # Training subprocess logs
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/                # QuixStreams RocksDB state
‚îÇ   ‚îî‚îÄ‚îÄ store_ml/                   # ML anomalies lakehouse
‚îÇ
‚îú‚îÄ‚îÄ docs/                           # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ DOCKER.md                   # Docker usage guide
‚îÇ   ‚îú‚îÄ‚îÄ MANUAL_SETUP.md             # Manual (non-Docker) setup
‚îÇ   ‚îú‚îÄ‚îÄ QUERY_CLASSIFICATION.md     # Classification methodology
‚îÇ   ‚îî‚îÄ‚îÄ QUERY_CLASSIFICATION_ANALYSIS.md  # Threshold derivation data
‚îÇ
‚îú‚îÄ‚îÄ pipeline/                       # Core pipeline modules
‚îÇ   ‚îú‚îÄ‚îÄ config.py                   # Shared configuration
‚îÇ   ‚îú‚îÄ‚îÄ consumer.py                 # Main consumer with ML inference + training
‚îÇ   ‚îú‚îÄ‚îÄ consumer_aggregate.py       # Stream analytics (classification logic)
‚îÇ   ‚îú‚îÄ‚îÄ consumer_ml.py              # Anomaly sink for dashboard
‚îÇ   ‚îú‚îÄ‚îÄ producer.py                 # Kafka producer with time acceleration
‚îÇ   ‚îî‚îÄ‚îÄ train_model.py              # Async training script
‚îÇ
‚îú‚îÄ‚îÄ kafka_stream/                   # Kafka infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose-local.yml    # Redpanda config (local)
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose-extern.yml   # Redpanda config (external network)
‚îÇ   ‚îú‚îÄ‚îÄ arrow_writer.py             # Feather+LZ4/ZSTD writer
‚îÇ   ‚îî‚îÄ‚îÄ anomalous_query_producer.py # Anomaly output producer
‚îÇ
‚îú‚îÄ‚îÄ outlier_tool/                   # ML library
‚îÇ   ‚îú‚îÄ‚îÄ redset_outlier_lib.py       # Core ML service (OutlierService)
‚îÇ   ‚îú‚îÄ‚îÄ business_features.py        # Feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ business_models.py          # Model artifacts & training
‚îÇ   ‚îú‚îÄ‚îÄ business_io.py              # Data loading & saving
‚îÇ   ‚îú‚îÄ‚îÄ business_pipeline.py        # ML pipeline stages
‚îÇ   ‚îî‚îÄ‚îÄ business_anomaly_logic.py   # Anomaly detection thresholds
‚îÇ
‚îú‚îÄ‚îÄ dashboard/                      # FastAPI + Preact dashboard
‚îÇ   ‚îú‚îÄ‚îÄ api.py                      # FastAPI backend (JSON API)
‚îÇ   ‚îú‚îÄ‚îÄ generate_html.py            # HTML generator (builds index.html)
‚îÇ   ‚îú‚îÄ‚îÄ index.html                  # Preact SPA (generated)
‚îÇ   ‚îú‚îÄ‚îÄ style.css                   # Dashboard styles
‚îÇ   ‚îú‚îÄ‚îÄ backend_connection.py       # Database connections
‚îÇ   ‚îú‚îÄ‚îÄ db_reader.py                # Data queries
‚îÇ   ‚îî‚îÄ‚îÄ recommendations/            # Remediation advice (markdown)
‚îÇ
‚îî‚îÄ‚îÄ assets/                         # Images and logos
```

---

## üö® Troubleshooting

### Training Fails
* Check `_data/logs/train_*.log` for errors
* Verify arrow files are readable: `import pyarrow.feather as feather; feather.read_feather("path")`
* Ensure training has enough data (at least N days configured)

### Low Throughput
* Use LZ4 compression (not ZSTD) for maximum write speed
* Check inference batch size (65k optimal)
* Run multiple consumer instances on partitioned topic
* Verify orjson is installed for fast JSON parsing

### Out of Memory
* Reduce inference batch size
* Reduce writer batch size
* Reduce Kafka buffer sizes

---

## üî¨ Reproducibility

> **Minimum requirement:** **8 GB of free RAM** to run the full pipeline (Redpanda + all consumers + producer + dashboard).

All development, testing, and benchmarking were performed on:

| Component | Specification |
|-----------|---------------|
| **Laptop** | ASUS ROG Zephyrus G16 (2024) ‚Äî GA605WI |
| **CPU** | AMD Ryzen AI 9 HX 370 ‚Äî 12 cores / 24 threads, up to 5.1 GHz |
| **RAM** | 32 GB LPDDR5X-7500 (soldered) |
| **GPU** | NVIDIA GeForce RTX 4070 Laptop ‚Äî 8 GB GDDR6 |
| **Storage** | 1 TB PCIe 4.0 NVMe SSD |
| **OS** | Windows 11 Home |

---

## üìù License

MIT License ‚Äî see [LICENSE](LICENSE).

---

## üôè Acknowledgments

Built with:

* [QuixStreams](https://github.com/quixio/quix-streams) ‚Äî Kafka streaming
* [PyArrow](https://arrow.apache.org/docs/python/) ‚Äî Feather format
* [ONNX Runtime](https://onnxruntime.ai/) ‚Äî ML inference
* [orjson](https://github.com/ijl/orjson) ‚Äî Fast JSON parsing
* [Redpanda](https://redpanda.com/) ‚Äî Kafka-compatible streaming
