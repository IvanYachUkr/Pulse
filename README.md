<!-- ============================================================
Pulse README with "safe" GitHub animations (SVG/GIF only, no JS)
Paste this as README.md
============================================================ -->

<p align="center">
  <img src="assets/cosmic_pulse.gif" alt="Pulse Logo" width="900">
</p>

<p align="center">
  <img
    src="https://capsule-render.vercel.app/api?type=soft&height=60&color=0:ff2d55,100:7a00ff&text=Pulse%20ML%20Pipeline:%20Real-time%20Outlier%20Detection&fontSize=36&fontColor=ffffff"
    alt="Pulse ML Pipeline â€“ Real-time Outlier Detection"
  />
</p>

<p align="center">
  <em>Don't let unoptimized queries and black-box algorithms swallow your cloud database budget like a black hole.<br/>
  Pulse brings real-time ML outlier detection to the edge of your data warehouse â€” so you can see what's really going on before it's too late.</em>
</p>

<br/>

Production-ready Kafka streaming pipeline with real-time ML inference and asynchronous model training.

## ğŸ’¡ Why Pulse?

- **ğŸš€ Production-Ready ONNX Inference** â€” Optimized ML inference with ONNX Runtime supporting both CPU and GPU execution providers
- **ğŸ“¦ Optimized File Format** â€” Apache Arrow Feather format with LZ4/ZSTD compression (47x faster writes than Parquet)
- **âš¡ Fully Async Training** â€” Model training runs in a detached subprocess (~90s average), never blocking real-time inference
- **ğŸ”„ Automatic Model Lifecycle** â€” Daily retraining with rolling window data; new models automatically loaded for next inference cycle
- **ğŸ“Š End-to-End Batch Processing** â€” Batched writes, reads, inference, and training for maximum throughput
- **ğŸ› ï¸ Optimized Libraries** â€” orjson (3.6x faster JSON), PyArrow (columnar storage), pandas (vectorized ops)
- **ğŸ“¡ Kafka Streaming** â€” Reliable, persistent message exchange with consumer groups and offset management
- **ğŸ“ˆ High Throughput** â€” 17,700 msg/s sustained throughput (300x+ real-time capacity)
- **ğŸ”§ Parallel Inference** â€” Configurable worker pool for multi-core CPU utilization
- **ğŸ³ One-Command Docker Deployment** â€” Full stack (Kafka + Pipeline + Dashboard) with `docker compose up`

---

## ğŸš€ Quick Start (Docker)

> **ğŸ“– Full Docker reference:** [docs/DOCKER.md](docs/DOCKER.md) â€” building locally, clean runs, environment variables, terminal options
>
> **ğŸ“– Manual setup (no Docker):** [docs/MANUAL_SETUP.md](docs/MANUAL_SETUP.md) â€” Python venv, individual component commands, run scripts

### 1. Clone and Get Data

```bash
git clone https://github.com/IvanYachUkr/Pulse.git
cd Pulse
```

Place a **date-sorted** Parquet file from the [Amazon Redset](https://github.com/amazon-science/redset) dataset at:

```
kafka_stream/data/sorted_4days.parquet
```

### 2. Start

```bash
docker compose up
```

This pulls the pre-built image, starts Kafka, and runs the entire pipeline. Wait 30â€“60 seconds for data to flow, then open the dashboard.

### 3. Access

| Service | URL |
|---------|-----|
| **Dashboard** | [http://localhost:8507](http://localhost:8507) |
| **Kafka Console** | [http://localhost:8080](http://localhost:8080) |

### 4. Stop

```bash
docker compose down
```

> **âš ï¸ Important:** Close the dashboard browser tab **before** restarting the pipeline from scratch. An open dashboard holds connections to SQLite/DuckDB databases inside the container, which can prevent proper cleanup on restart.

---

## ğŸ¯ Overview

This pipeline processes query events from Kafka, detects outliers using ONNX models, and automatically retrains models daily. Optimized for high throughput with Apache Arrow Feather format achieving **47x faster writes** than Parquet.

### Data Flow

```
Kafka â†’ Consumer â†’ Feather Writer â†’ Daily Files (.arrow)
                 â†“
              Inference (ONNX)
                 â†“
           Anomaly Detection
                 
Daily Files â†’ Async Training (subprocess) â†’ New Models â†’ Next Day Inference
```

### Key Features

* **Real-time inference:** Batch inference with ONNX runtime
* **Asynchronous training:** Non-blocking model retraining in subprocess (~90s average)
* **High performance:** 17,700 msg/s throughput (300x+ real-time capacity)
* **Optimized storage:** Apache Arrow Feather format (47x faster writes than Parquet-gzip)
* **Compression options:** LZ4 (maximum speed) or ZSTD (2x better compression)
* **Fast JSON parsing:** orjson (3.6x faster than stdlib)

---

## ğŸ“Š Query Classification

Every query is classified into one of five categories based on its performance bottleneck:

| Class | What It Captures | Key Threshold |
|-------|-----------------|---------------|
| **Network-bound** | COPY/UNLOAD data transfers | exec > 500ms |
| **CPU-bound** | Complex compute-heavy queries | exec/scan ratio > 20, exec > 1s |
| **IO-bound** | Large scans bottlenecked by disk | scan â‰¥ 2 GB, ratio < 0.061 |
| **Queue/WLM-bound** | Simple queries stuck waiting | queue â‰¥ exec, complexity < 4 |
| **Normal** | No dominant bottleneck | Everything else |

All thresholds are **data-derived** from behavioral regime analysis of 433M queries â€” not hardcoded assumptions. See **[QUERY_CLASSIFICATION.md](docs/QUERY_CLASSIFICATION.md)** for the full methodology, transition points, and validation results.

---

## ğŸ“Š Dashboard Architecture

The dashboard is a **decoupled SPA** built with a **FastAPI JSON backend** and a **Preact + HTM** client â€” no build tools, no bundler, no `node_modules`.

### Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Backend** | FastAPI + Uvicorn | JSON API serving dashboard data from SQLite/DuckDB |
| **Frontend** | Preact + HTM (via CDN) | Reactive UI with hooks, delivered as a single HTML file |
| **Charts** | Chart.js (via CDN) | Interactive time-series and classification charts |
| **Styling** | Vanilla CSS (custom) | Dark-mode glassmorphism design with CSS variables |

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/status` | GET | Pipeline component health status |
| `/api/instances` | GET | Available cluster instance IDs |
| `/api/instances/critical` | GET | Instances flagged as critical |
| `/api/metrics?ids=...` | GET | Aggregated performance metrics |
| `/api/classification/chart?ids=...` | GET | Query classification time-series |
| `/api/classification/table?ids=...` | GET | Query classification breakdown |
| `/api/anomalies?ids=...` | GET | Detected anomalous queries |
| `/api/critical/types?ids=...` | GET | Critical problem type distribution |
| `/api/recommendations/{type}` | GET | Auto-generated remediation advice |

### Key Features

* **Multi-instance selection** â€” Select/deselect individual instances, use "Select All" / "Critical Only" shortcuts
* **Time window toggle** â€” Switch between 24h and 1-week views
* **Real-time status indicators** â€” Live health checks for stream analytics, anomaly detection, and database
* **Metric cards** â€” Score, queue time, spillage, anomaly count with trend indicators
* **Interactive charts** â€” Stacked area chart for query classification over time
* **Anomaly table** â€” Detected anomalous queries with severity highlighting
* **Recommendations panel** â€” Context-aware remediation advice based on dominant problem types

---

## ğŸ“Š Performance

### Overall Pipeline

| Component | Throughput | Notes |
|-----------|-----------|-------|
| **Producer** | 31-38k msg/s | Consistent performance |
| **Consumer (with inference)** | 17,700 msg/s | 4 workers, batch 65k |
| **Consumer (write only)** | 21,600 msg/s | Excluding inference |
| **Training** | ~90s/day | 400k rows per day (async) |

**Real-time capacity:** ~36 msg/s (300x+ speedup exceeds requirements)

### Inference Parallelization Benchmark (600k messages)

| Workers | CPU Throughput | GPU Throughput |
|---------|---------------|---------------|
| **1** | 11,739 msg/s | 10,229 msg/s |
| **4** | **12,864 msg/s** â­ | 11,248 msg/s |
| **8** | 12,724 msg/s | â€” |

**Recommendation:** Use `--inference-workers 4` for optimal performance (CPU).

> âš ï¸ **GPU Note:** Tested with RTX 4070. GPU is **SLOWER** than CPU for this workload! The ONNX Isolation Forest model requires CPUâ†”GPU data transfers (memcpy) that negate any compute gains. Use CPU.

---

## ğŸ”§ Design Decisions & Benchmarks

### Apache Arrow Feather Format (47x Faster Writes)

| Format | Write Speed | Read Speed | File Size |
|--------|------------|-----------|-----------|
| Parquet (gzip) | 1x (baseline) | **5x faster** | **Smallest** |
| **Feather (LZ4)** | **47x faster** | 1x | 7x larger |

**Writes happen in the main consumer loop** (directly impacts throughput). **Reads happen asynchronously** in a subprocess. Therefore, write speed is critical.

### Compression Options (LZ4 vs ZSTD)

| Compression | Write Speed | File Size | Use Case |
|-------------|-----------|-----------|----------|
| **LZ4** (default) | Fastest | Larger (2.3x ratio) | Real-time streaming |
| **ZSTD** | 40% slower | **2x smaller** (5.6x ratio) | Disk-constrained, archival |

### JSON Parsing with orjson (3.6x Faster)

| Library | Speed | Improvement |
|---------|-------|-------------|
| json (stdlib) | 1x (baseline) | - |
| **orjson** | **3.6x faster** | 260% faster |

---

## ğŸ“ Project Structure

```
Pulse/
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ docker-compose.yml              # Full stack Docker setup
â”œâ”€â”€ Dockerfile                      # Pipeline container image
â”œâ”€â”€ docker-entrypoint.sh            # Container startup script
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ run_pipeline.bat / .sh          # Manual pipeline launchers
â”œâ”€â”€ stop_pipeline.bat / .sh         # Manual pipeline stoppers
â”‚
â”œâ”€â”€ docs/                           # Documentation
â”‚   â”œâ”€â”€ DOCKER.md                   # Docker usage guide
â”‚   â”œâ”€â”€ MANUAL_SETUP.md             # Manual (non-Docker) setup
â”‚   â””â”€â”€ QUERY_CLASSIFICATION.md     # Classification methodology
â”‚
â”œâ”€â”€ pipeline/                       # Core pipeline modules
â”‚   â”œâ”€â”€ config.py                   # Shared configuration
â”‚   â”œâ”€â”€ consumer.py                 # Main consumer with ML inference + training
â”‚   â”œâ”€â”€ consumer_aggregate.py       # Stream analytics (classification logic)
â”‚   â”œâ”€â”€ consumer_ml.py              # Anomaly sink for dashboard
â”‚   â”œâ”€â”€ producer.py                 # Kafka producer with time acceleration
â”‚   â””â”€â”€ train_model.py              # Async training script
â”‚
â”œâ”€â”€ kafka_stream/                   # Kafka infrastructure
â”‚   â”œâ”€â”€ docker-compose-local.yml    # Redpanda config (local)
â”‚   â”œâ”€â”€ docker-compose-extern.yml   # Redpanda config (external network)
â”‚   â”œâ”€â”€ arrow_writer.py             # Feather+LZ4/ZSTD writer
â”‚   â”œâ”€â”€ anomalous_query_producer.py # Anomaly output producer
â”‚   â””â”€â”€ data/                       # Input data files (parquet)
â”‚
â”œâ”€â”€ outlier_tool/                   # ML library
â”‚   â”œâ”€â”€ redset_outlier_lib.py       # Core ML service (OutlierService)
â”‚   â”œâ”€â”€ business_features.py        # Feature engineering
â”‚   â”œâ”€â”€ business_models.py          # Model artifacts & training
â”‚   â”œâ”€â”€ business_io.py              # Data loading & saving
â”‚   â”œâ”€â”€ business_pipeline.py        # ML pipeline stages
â”‚   â”œâ”€â”€ business_anomaly_logic.py   # Anomaly detection thresholds
â”‚   â””â”€â”€ ML_README.md                # ML library documentation
â”‚
â”œâ”€â”€ dashboard/                      # FastAPI + Preact dashboard
â”‚   â”œâ”€â”€ api.py                      # FastAPI backend (JSON API)
â”‚   â”œâ”€â”€ generate_html.py            # HTML generator (builds index.html)
â”‚   â”œâ”€â”€ index.html                  # Preact SPA (generated)
â”‚   â”œâ”€â”€ style.css                   # Dashboard styles
â”‚   â”œâ”€â”€ backend_connection.py       # Database connections
â”‚   â”œâ”€â”€ db_reader.py                # Data queries
â”‚   â”œâ”€â”€ status_monitor.py           # System status
â”‚   â”œâ”€â”€ databases/                  # SQLite + DuckDB files
â”‚   â”œâ”€â”€ lakehouse_ml/               # ML anomalies lakehouse
â”‚   â””â”€â”€ recommendations/            # Auto-generated recommendations
â”‚
â”œâ”€â”€ assets/                         # Images and logos
â”œâ”€â”€ business_case_analysis/         # Business analysis scripts
â”œâ”€â”€ data/db/                        # Output: daily Arrow shard files
â”œâ”€â”€ out_models/                     # Output: trained ONNX models
â””â”€â”€ training_logs/                  # Output: training subprocess logs
```

---

## ğŸš¨ Troubleshooting

### Training Fails
* Check `training_logs/train_*.log` for errors
* Verify arrow files are readable: `import pyarrow.feather as feather; feather.read_feather("path")`
* Ensure training has enough data (at least N days configured)

### Low Throughput
* Use LZ4 compression (not ZSTD) for maximum write speed
* Check inference batch size (65k optimal)
* Run multiple consumer instances on partitioned topic
* Verify orjson is installed for fast JSON parsing

### Out of Memory
* Reduce inference batch size
* Reduce writer batch size
* Reduce Kafka buffer sizes

---

## ğŸ“ License

This project is licensed under the MIT License - see below:

```
MIT License

Copyright (c) 2026 Pulse

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸ™ Acknowledgments

Built with:

* [quixstreams](https://github.com/quixio/quix-streams) - Kafka streaming
* [PyArrow](https://arrow.apache.org/docs/python/) - Feather format
* [ONNX Runtime](https://onnxruntime.ai/) - ML inference
* [orjson](https://github.com/ijl/orjson) - Fast JSON parsing
* [Redpanda](https://redpanda.com/) - Kafka-compatible streaming
